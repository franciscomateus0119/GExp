{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8191e9ff-5fda-43ea-8676-eaadee7fcfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mateus\\anaconda3\\Lib\\site-packages\\alibi\\explainers\\cem.py:35: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pulp import *\n",
    "from pulp import LpProblem, LpVariable, LpMinimize, LpInteger, lpSum, value, LpBinary,LpStatusOptimal\n",
    "import pulp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overwriting previously set objective.\")\n",
    "import utility\n",
    "import docplex.mp.model\n",
    "import docplex\n",
    "import docplex_explainer\n",
    "import mymetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import dill\n",
    "from alibi.explainers import AnchorTabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d45cc7-bde4-478e-bd98-777a198d5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'Iris':\n",
    "        # Load Dataset\n",
    "        dataset = datasets.load_iris()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        # Check if binary targets\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Banknote':\n",
    "        # Load Dataset\n",
    "        df = pd.read_csv('./datasets/banknote_authentication.csv')\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Blood_Transfusion':\n",
    "        df = pd.read_csv('./datasets/blood_transfusion.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Breast_Cancer':\n",
    "        dataset = datasets.load_breast_cancer()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Climate':\n",
    "        df = pd.read_csv('./datasets/climate_model_simulation_crashes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Glass':\n",
    "        df = pd.read_csv('./datasets/glass.csv')\n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Ionosphere':\n",
    "        df = pd.read_csv('./datasets/ionosphere.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Modeling':\n",
    "        df = pd.read_csv('./datasets/User_Knowledge_Modeling.csv')\n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x == 'Low' else 0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Parkinsons':\n",
    "        df = pd.read_csv('./datasets/parkinsons.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Pima':\n",
    "        df = pd.read_csv('./datasets/diabetes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Sonar':\n",
    "        df = pd.read_csv('./datasets/sonar.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Wine':\n",
    "        dataset = datasets.load_wine()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Vertebral-Column':\n",
    "        dataset_name = 'Vertebral-Column'\n",
    "        df = pd.read_csv('./datasets/column_2C.dat', sep=\" \", names=['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis','target'])\n",
    "        df['target']=np.where(df['target']=='AB',1,0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "        \n",
    "    else:\n",
    "        print(\"Incorrect dataset name\")\n",
    "\n",
    "def parse_explanation(explanation, feature_names, epsilon=1e-6):\n",
    "    bounds = [[0, 1] for _ in range(len(feature_names))]\n",
    "    conditions = explanation\n",
    "\n",
    "    for condition in conditions:\n",
    "        condition_no_space = condition.replace(' ', '')  # for regex matching\n",
    "        # Check for double inequality\n",
    "        match = re.match(r'(\\d+\\.?\\d*)\\s*(<|<=)\\s*([^\\s<>]+)\\s*(<|<=)\\s*(\\d+\\.?\\d*)', condition_no_space)\n",
    "        \n",
    "        if match:\n",
    "            value_1, op1, feature_token, op2, value_2 = match.groups()\n",
    "            value_1 = float(value_1)\n",
    "            value_2 = float(value_2)\n",
    "            lower_bound = value_1 if op1 == '<=' else value_1 + epsilon\n",
    "            upper_bound = value_2 if op2 == '<=' else value_2\n",
    "            upper_bound = upper_bound if op2 == '<=' else upper_bound - epsilon\n",
    "\n",
    "            for idx, feature in enumerate(feature_names):\n",
    "                if feature.replace(\" \", \"\") in feature_token:\n",
    "                    bounds[idx] = [lower_bound, upper_bound]\n",
    "                    break\n",
    "            continue  # go to next condition\n",
    "\n",
    "        # Fallback to single operator logic\n",
    "        for idx, feature in enumerate(feature_names):\n",
    "            if feature in condition:\n",
    "                cond_clean = condition.replace('<=', ' LESS_EQUAL ').replace('>=', ' GREATER_EQUAL ')\n",
    "                cond_clean = cond_clean.replace('<', ' < ').replace('>', ' > ')\n",
    "                tokens = cond_clean.split()\n",
    "\n",
    "                tokens = ['<=' if token == 'LESS_EQUAL' else token for token in tokens]\n",
    "                tokens = ['>=' if token == 'GREATER_EQUAL' else token for token in tokens]\n",
    "\n",
    "                operator = None\n",
    "                operator_pos = None\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if token in ['>', '>=', '<', '<=']:\n",
    "                        operator = token\n",
    "                        operator_pos = i\n",
    "                        break\n",
    "\n",
    "                value = None\n",
    "                if operator is not None and operator_pos is not None:\n",
    "                    for i in range(operator_pos + 1, len(tokens)):\n",
    "                        try:\n",
    "                            value = float(tokens[i])\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "                if value is not None:\n",
    "                    if operator == '>':\n",
    "                        bounds[idx] = [value + epsilon, 1]\n",
    "                    elif operator == '>=':\n",
    "                        bounds[idx] = [value, 1]\n",
    "                    elif operator == '<':\n",
    "                        bounds[idx] = [0, value - epsilon]\n",
    "                    elif operator == '<=':\n",
    "                        bounds[idx] = [0, value]\n",
    "                else:\n",
    "                    print(f\"Could not extract numeric value from condition: '{condition}'\")\n",
    "\n",
    "    return np.array(bounds)\n",
    "def train_anchors(dataset_name):\n",
    "    print(dataset_name)\n",
    "    clf = joblib.load(f'models/{dataset_name}_svm_model.pkl')\n",
    "    print(f'Loaded model')\n",
    "    X_train, X_test, y_train, y_test,feature_names, class_names = load_data(dataset_name)\n",
    "    if 'target' in feature_names:\n",
    "        feature_names = feature_names[feature_names != 'target']\n",
    "    print(f'feature names:\\n {feature_names}')\n",
    "    print(f'class names:\\n {class_names}')\n",
    "    predict_fn = lambda x: clf.predict(x)\n",
    "    explainer = AnchorTabular(predict_fn, feature_names)\n",
    "    explainer.fit(X_train, disc_perc=(25, 50, 75))\n",
    "    return explainer, feature_names, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "66cc0668-ca10-43c9-af74-516342fef139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Sonar' #Iris, Wine, Vertebral-Column, Pima, Parkinsons, Breast_Cancer, Blood_Transfusion, Ionosphere, Glass, Climate, Modeling, Banknote, Sonar\n",
    "path_anchors = f'./Anchors_results/{dataset_name}_results.csv' \n",
    "path_twostep = dataset_name+ '_results/results_0.25.csv'\n",
    "path_twostep_brute = dataset_name+ '_results/raw_metric_data_0.25.csv'\n",
    "np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bbe726b0-59ab-4c42-b659-b682a30b7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_brute_results = pd.read_csv(path_anchors)\n",
    "twostep_results = pd.read_csv(path_twostep)\n",
    "twostep_results_brute = pd.read_csv(path_twostep_brute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "664e6419-65d4-4ac8-827a-3b8f5046fa74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_onestep</th>\n",
       "      <th>times_twostep</th>\n",
       "      <th>feature_sizes_onestep</th>\n",
       "      <th>feature_sizes_twostep</th>\n",
       "      <th>rsum_onestep</th>\n",
       "      <th>rsum_twostep</th>\n",
       "      <th>coverage_onestep</th>\n",
       "      <th>coverage_twostep</th>\n",
       "      <th>time_relative_%</th>\n",
       "      <th>sizes_relative_%</th>\n",
       "      <th>rsum_relative_%</th>\n",
       "      <th>coverage_relative_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192907</td>\n",
       "      <td>0.299260</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>32.666765</td>\n",
       "      <td>32.666765</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55.131983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194326</td>\n",
       "      <td>0.317852</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>32.101711</td>\n",
       "      <td>32.119355</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.566500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054963</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174175</td>\n",
       "      <td>0.271793</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>34.848134</td>\n",
       "      <td>34.848546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56.045368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.188589</td>\n",
       "      <td>0.299771</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>33.563811</td>\n",
       "      <td>33.585372</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.954795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165660</td>\n",
       "      <td>0.264267</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>34.533158</td>\n",
       "      <td>34.531416</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59.523772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005044</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.178652</td>\n",
       "      <td>0.275697</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>29.749029</td>\n",
       "      <td>29.750363</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.320233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.184798</td>\n",
       "      <td>0.288714</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>28.727888</td>\n",
       "      <td>28.729458</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56.232011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.186537</td>\n",
       "      <td>0.293915</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>30.002368</td>\n",
       "      <td>30.007849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.564267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018266</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.296630</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>27.942881</td>\n",
       "      <td>27.951010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.689521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029089</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.187474</td>\n",
       "      <td>0.302834</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>27.946191</td>\n",
       "      <td>27.947340</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61.534285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_onestep  times_twostep  feature_sizes_onestep  \\\n",
       "0         0.192907       0.299260                     53   \n",
       "1         0.194326       0.317852                     56   \n",
       "2         0.174175       0.271793                     47   \n",
       "3         0.188589       0.299771                     53   \n",
       "4         0.165660       0.264267                     43   \n",
       "..             ...            ...                    ...   \n",
       "151       0.178652       0.275697                     49   \n",
       "152       0.184798       0.288714                     52   \n",
       "153       0.186537       0.293915                     51   \n",
       "154       0.188110       0.296630                     52   \n",
       "155       0.187474       0.302834                     53   \n",
       "\n",
       "     feature_sizes_twostep  rsum_onestep  rsum_twostep  coverage_onestep  \\\n",
       "0                       53     32.666765     32.666765                 1   \n",
       "1                       56     32.101711     32.119355                 1   \n",
       "2                       47     34.848134     34.848546                 1   \n",
       "3                       53     33.563811     33.585372                 1   \n",
       "4                       43     34.533158     34.531416                 1   \n",
       "..                     ...           ...           ...               ...   \n",
       "151                     49     29.749029     29.750363                 1   \n",
       "152                     52     28.727888     28.729458                 1   \n",
       "153                     51     30.002368     30.007849                 1   \n",
       "154                     52     27.942881     27.951010                 1   \n",
       "155                     53     27.946191     27.947340                 1   \n",
       "\n",
       "     coverage_twostep  time_relative_%  sizes_relative_%  rsum_relative_%  \\\n",
       "0                   1        55.131983               0.0         0.000000   \n",
       "1                   1        63.566500               0.0         0.054963   \n",
       "2                   1        56.045368               0.0         0.001181   \n",
       "3                   1        58.954795               0.0         0.064237   \n",
       "4                   1        59.523772               0.0        -0.005044   \n",
       "..                ...              ...               ...              ...   \n",
       "151                 1        54.320233               0.0         0.004482   \n",
       "152                 1        56.232011               0.0         0.005467   \n",
       "153                 1        57.564267               0.0         0.018266   \n",
       "154                 1        57.689521               0.0         0.029089   \n",
       "155                 1        61.534285               0.0         0.004112   \n",
       "\n",
       "     coverage_relative_%  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "..                   ...  \n",
       "151                  0.0  \n",
       "152                  0.0  \n",
       "153                  0.0  \n",
       "154                  0.0  \n",
       "155                  0.0  \n",
       "\n",
       "[156 rows x 12 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twostep_results_brute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a1439939-0672-4a21-85b7-7772f3be38c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coverage</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "      <th>rsum</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.759618</td>\n",
       "      <td>54.660000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4.709863</td>\n",
       "      <td>56.880000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.388251</td>\n",
       "      <td>57.539996</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.644434</td>\n",
       "      <td>55.979998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.981253</td>\n",
       "      <td>57.539996</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.379192</td>\n",
       "      <td>57.019996</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.139333</td>\n",
       "      <td>58.089996</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.070577</td>\n",
       "      <td>57.519994</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.091458</td>\n",
       "      <td>55.730000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4.138346</td>\n",
       "      <td>56.030000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     coverage  errors      time       rsum  size\n",
       "0           3       0  3.759618  54.660000     3\n",
       "1           8       3  4.709863  56.880000     3\n",
       "2           1       0  2.388251  57.539996     2\n",
       "3           1       0  3.644434  55.979998     3\n",
       "4           1       0  2.981253  57.539996     2\n",
       "..        ...     ...       ...        ...   ...\n",
       "151         1       0  3.379192  57.019996     3\n",
       "152         1       0  5.139333  58.089996     3\n",
       "153         1       0  4.070577  57.519994     3\n",
       "154         4       0  4.091458  55.730000     3\n",
       "155        10       1  4.138346  56.030000     3\n",
       "\n",
       "[156 rows x 5 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors_brute_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2054936-c6bf-456e-93a2-2eb449236e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_anchors = anchors_brute_results['time'].values\n",
    "coverage_anchors = anchors_brute_results['coverage'].values\n",
    "errors_anchors = anchors_brute_results['errors'].values\n",
    "rsum_anchors = anchors_brute_results['rsum'].values\n",
    "sizes_anchors = anchors_brute_results['size'].values\n",
    "\n",
    "times_twostep = twostep_results_brute['times_twostep'].values\n",
    "coverage_twostep = twostep_results_brute['coverage_twostep'].values\n",
    "rsum_twostep = twostep_results_brute['rsum_twostep'].values\n",
    "feature_sizes_twostep = twostep_results_brute['feature_sizes_twostep'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "da7eca8c-fdd3-4d7d-aee1-0831ac43c975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>ANCHORS_MEAN</th>\n",
       "      <th>ANCHORS_STD</th>\n",
       "      <th>TWOSTEP_MEAN</th>\n",
       "      <th>TWOSTEP_STD</th>\n",
       "      <th>MEAN_DIFF_%</th>\n",
       "      <th>STD_DIFF_%</th>\n",
       "      <th>POINTWISE_MEAN_%</th>\n",
       "      <th>POINTWISE_STD_%</th>\n",
       "      <th>ANCHORS_ERROR_RATE_MEAN%</th>\n",
       "      <th>ANCHORS_ERROR_RATE_STD%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Time</td>\n",
       "      <td>4.138793</td>\n",
       "      <td>0.816539</td>\n",
       "      <td>0.297104</td>\n",
       "      <td>0.019707</td>\n",
       "      <td>-92.821480</td>\n",
       "      <td>-97.586524</td>\n",
       "      <td>-92.493932</td>\n",
       "      <td>1.833161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Size</td>\n",
       "      <td>2.743590</td>\n",
       "      <td>0.436651</td>\n",
       "      <td>51.647436</td>\n",
       "      <td>3.800817</td>\n",
       "      <td>1782.476636</td>\n",
       "      <td>770.447517</td>\n",
       "      <td>1840.277778</td>\n",
       "      <td>391.709665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ranges_Sum</td>\n",
       "      <td>57.070382</td>\n",
       "      <td>1.107677</td>\n",
       "      <td>31.856469</td>\n",
       "      <td>2.598200</td>\n",
       "      <td>-44.180381</td>\n",
       "      <td>134.563070</td>\n",
       "      <td>-44.158748</td>\n",
       "      <td>4.679504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coverage</td>\n",
       "      <td>3.243590</td>\n",
       "      <td>4.417871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-69.169960</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-32.443540</td>\n",
       "      <td>38.076245</td>\n",
       "      <td>2.075322</td>\n",
       "      <td>7.219884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric  ANCHORS_MEAN  ANCHORS_STD  TWOSTEP_MEAN  TWOSTEP_STD  \\\n",
       "0        Time      4.138793     0.816539      0.297104     0.019707   \n",
       "1        Size      2.743590     0.436651     51.647436     3.800817   \n",
       "2  Ranges_Sum     57.070382     1.107677     31.856469     2.598200   \n",
       "3    Coverage      3.243590     4.417871      1.000000     0.000000   \n",
       "\n",
       "   MEAN_DIFF_%  STD_DIFF_%  POINTWISE_MEAN_%  POINTWISE_STD_%  \\\n",
       "0   -92.821480  -97.586524        -92.493932         1.833161   \n",
       "1  1782.476636  770.447517       1840.277778       391.709665   \n",
       "2   -44.180381  134.563070        -44.158748         4.679504   \n",
       "3   -69.169960 -100.000000        -32.443540        38.076245   \n",
       "\n",
       "   ANCHORS_ERROR_RATE_MEAN%  ANCHORS_ERROR_RATE_STD%  \n",
       "0                       NaN                      NaN  \n",
       "1                       NaN                      NaN  \n",
       "2                       NaN                      NaN  \n",
       "3                  2.075322                 7.219884  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# Compute means and standard deviations\n",
    "def compute_mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "# Compute relative percentage differences\n",
    "def relative_percentage_diff(new, old):\n",
    "    if np.any(old == 0):\n",
    "        print(f'Warning: found possible division by zero')\n",
    "        return np.where(old != 0, ((new - old) / old) * 100, np.nan)\n",
    "    return ((new - old) / old) * 100\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all lists are NumPy arrays\n",
    "times_twostep = np.array(times_twostep)\n",
    "coverage_twostep = np.array(coverage_twostep)\n",
    "sizes_anchors = np.array(sizes_anchors)\n",
    "feature_sizes_twostep = np.array(feature_sizes_twostep)\n",
    "rsum_anchors = np.array(rsum_anchors)\n",
    "rsum_twostep = np.array(rsum_twostep)\n",
    "\n",
    "\n",
    "\n",
    "# Compute means and standard deviations\n",
    "(time_mean_anchors, time_std_anchors) = compute_mean_std(times_anchors)\n",
    "(time_mean_twostep, time_std_twostep) = compute_mean_std(times_twostep)\n",
    "\n",
    "(coverage_mean_anchors, coverage_std_anchors) = compute_mean_std(coverage_anchors)\n",
    "(coverage_mean_twostep, coverage_std_twostep) = compute_mean_std(coverage_twostep)\n",
    "\n",
    "(sizes_mean_anchors, sizes_std_anchors) = compute_mean_std(sizes_anchors)\n",
    "(sizes_mean_twostep, sizes_std_twostep) = compute_mean_std(feature_sizes_twostep)\n",
    "\n",
    "(rsum_mean_anchors, rsum_std_anchors) = compute_mean_std(rsum_anchors)\n",
    "(rsum_mean_twostep, rsum_std_twostep) = compute_mean_std(rsum_twostep)\n",
    "\n",
    "# Compute relative percentage differences (Mean & Std)\n",
    "time_mean_diff = relative_percentage_diff(time_mean_twostep, time_mean_anchors)\n",
    "coverage_mean_diff = relative_percentage_diff(coverage_mean_twostep, coverage_mean_anchors)\n",
    "\n",
    "time_std_diff = relative_percentage_diff(time_std_twostep, time_std_anchors)\n",
    "coverage_std_diff = relative_percentage_diff(coverage_std_twostep, coverage_std_anchors)\n",
    "\n",
    "sizes_mean_diff = relative_percentage_diff(sizes_mean_twostep, sizes_mean_anchors)\n",
    "sizes_std_diff = relative_percentage_diff(sizes_std_twostep, sizes_std_anchors)\n",
    "\n",
    "rsum_mean_diff = relative_percentage_diff(rsum_mean_twostep, rsum_mean_anchors)\n",
    "rsum_std_diff = relative_percentage_diff(rsum_std_twostep, rsum_std_anchors)\n",
    "\n",
    "# Compute pointwise relative differences\n",
    "time_relative_pointwise = relative_percentage_diff(times_twostep, times_anchors)\n",
    "coverage_relative_pointwise = relative_percentage_diff(coverage_twostep, coverage_anchors)\n",
    "\n",
    "sizes_relative_pointwise = relative_percentage_diff(feature_sizes_twostep, sizes_anchors)\n",
    "rsum_relative_pointwise = relative_percentage_diff(rsum_twostep, rsum_anchors)\n",
    "\n",
    "# Compute pointwise means\n",
    "time_relative_mean = np.mean(time_relative_pointwise) \n",
    "coverage_relative_mean = np.mean(coverage_relative_pointwise)\n",
    "sizes_relative_mean = np.mean(sizes_relative_pointwise)\n",
    "rsum_relative_mean = np.mean(rsum_relative_pointwise)\n",
    "\n",
    "# Compute pointwise standard deviations\n",
    "time_relative_std = np.std(time_relative_pointwise) \n",
    "coverage_relative_std = np.std(coverage_relative_pointwise)\n",
    "sizes_relative_std = np.std(sizes_relative_pointwise)\n",
    "rsum_relative_std = np.std(rsum_relative_pointwise)\n",
    "\n",
    "# Organize Data\n",
    "all_metrics_data = all_metrics_data = {\n",
    "    'Metric': ['Time', 'Size', 'Ranges_Sum', 'Coverage'],\n",
    "    'ANCHORS_MEAN': [time_mean_anchors, sizes_mean_anchors, rsum_mean_anchors, coverage_mean_anchors],\n",
    "    'ANCHORS_STD': [time_std_anchors, sizes_std_anchors, rsum_std_anchors, coverage_std_anchors],\n",
    "    'TWOSTEP_MEAN': [time_mean_twostep, sizes_mean_twostep, rsum_mean_twostep, coverage_mean_twostep],\n",
    "    'TWOSTEP_STD': [time_std_twostep, sizes_std_twostep, rsum_std_twostep, coverage_std_twostep],\n",
    "    'MEAN_DIFF_%': [time_mean_diff, sizes_mean_diff, rsum_mean_diff, coverage_mean_diff],\n",
    "    'STD_DIFF_%': [time_std_diff, sizes_std_diff, rsum_std_diff, coverage_std_diff],\n",
    "    'POINTWISE_MEAN_%': [time_relative_mean, sizes_relative_mean, rsum_relative_mean, coverage_relative_mean],\n",
    "    'POINTWISE_STD_%': [time_relative_std, sizes_relative_std, rsum_relative_std, coverage_relative_std],\n",
    "    'ANCHORS_ERROR_RATE_MEAN%': [None, None, None, np.mean(errors_anchors/coverage_anchors)*100],\n",
    "    'ANCHORS_ERROR_RATE_STD%': [None, None, None, np.std(errors_anchors/coverage_anchors)*100],\n",
    "    \n",
    "}\n",
    "# Display and save\n",
    "all_metrics_df = pd.DataFrame(all_metrics_data)\n",
    "display(all_metrics_df)\n",
    "print(errors_anchors.sum())\n",
    "all_metrics_df.to_csv(f'Anchors_vs_Twostep_Results/Original_{dataset_name}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1421f8-d3cb-4968-a086-04e44b6cb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "#df_artificial = pd.read_csv('datasets/artificial/'+f'{dataset_name}_artificial.csv')\n",
    "clf = joblib.load(f'models/{dataset_name}_svm_model.pkl')\n",
    "loaded_bounds = np.load(f'models/{dataset_name}_data_bounds.npz')\n",
    "lower_bound = loaded_bounds['lower_bound']\n",
    "upper_bound = loaded_bounds['upper_bound']\n",
    "np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12ef063-3dd6-465c-b6e9-fd7965cc2d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonar\n",
      "Loaded model\n",
      "Original Targets:  [0. 1.] \n",
      "Desired Targets: [0,1]\n",
      "Is original the desired [0, 1]?  True\n",
      "feature names:\n",
      " ['f1' 'f2' 'f3' 'f4' 'f5' 'f6' 'f7' 'f8' 'f9' 'f10' 'f11' 'f12' 'f13'\n",
      " 'f14' 'f15' 'f16' 'f17' 'f18' 'f19' 'f20' 'f21' 'f22' 'f23' 'f24' 'f25'\n",
      " 'f26' 'f27' 'f28' 'f29' 'f30' 'f31' 'f32' 'f33' 'f34' 'f35' 'f36' 'f37'\n",
      " 'f38' 'f39' 'f40' 'f41' 'f42' 'f43' 'f44' 'f45' 'f46' 'f47' 'f48' 'f49'\n",
      " 'f50' 'f51' 'f52' 'f53' 'f54' 'f55' 'f56' 'f57' 'f58' 'f59' 'f60']\n",
      "class names:\n",
      " [0 1]\n"
     ]
    }
   ],
   "source": [
    "d = 0.5\n",
    "p = 0.25\n",
    "num_instances=100\n",
    "X_test = pd.read_csv(f'{dataset_name}_results/{dataset_name}_X_test.csv')\n",
    "epsilon = 1e-6\n",
    "anchors_explainer, feature_names,class_names = train_anchors(dataset_name)\n",
    "anchors_explainer.save(f'Anchors_explainers/{dataset_name}_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7131b7-8db9-4f67-bd01-cf7d2ae57889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>ANCHORS_MEAN</th>\n",
       "      <th>ANCHORS_STD</th>\n",
       "      <th>TWOSTEP_MEAN</th>\n",
       "      <th>TWOSTEP_STD</th>\n",
       "      <th>MEAN_DIFF_%</th>\n",
       "      <th>STD_DIFF_%</th>\n",
       "      <th>POINTWISE_MEAN_%</th>\n",
       "      <th>POINTWISE_STD_%</th>\n",
       "      <th>ANCHORS_ERROR_RATE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coverage</td>\n",
       "      <td>3.602564</td>\n",
       "      <td>5.504869</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-72.241993</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-28.954462</td>\n",
       "      <td>38.099699</td>\n",
       "      <td>3.60777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Metric  ANCHORS_MEAN  ANCHORS_STD  TWOSTEP_MEAN  TWOSTEP_STD  \\\n",
       "0  Coverage      3.602564     5.504869           1.0          0.0   \n",
       "\n",
       "   MEAN_DIFF_%  STD_DIFF_%  POINTWISE_MEAN_%  POINTWISE_STD_%  \\\n",
       "0   -72.241993      -100.0        -28.954462        38.099699   \n",
       "\n",
       "   ANCHORS_ERROR_RATE%  \n",
       "0              3.60777  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coverage_anchors</th>\n",
       "      <th>coverage_twostep</th>\n",
       "      <th>coverage_relative_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     coverage_anchors  coverage_twostep  coverage_relative_%\n",
       "0                   1                 1             0.000000\n",
       "1                   3                 1           -66.666667\n",
       "2                   1                 1             0.000000\n",
       "3                   1                 1             0.000000\n",
       "4                   1                 1             0.000000\n",
       "..                ...               ...                  ...\n",
       "151                 7                 1           -85.714286\n",
       "152                 5                 1           -80.000000\n",
       "153                 1                 1             0.000000\n",
       "154                 1                 1             0.000000\n",
       "155                 1                 1             0.000000\n",
       "\n",
       "[156 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "twostep_coverages = []\n",
    "anchors_coverages = []\n",
    "anchors_errors = []\n",
    "anchors_rsums = []\n",
    "anchors_sizes = []\n",
    "for instance in X_test.values:\n",
    "    #print(instance)\n",
    "    artificial_lower_bounds = instance-d\n",
    "    artificial_lower_bounds[artificial_lower_bounds<lower_bound] = lower_bound\n",
    "    artificial_upper_bounds = instance+d\n",
    "    artificial_upper_bounds[artificial_upper_bounds>upper_bound] = upper_bound\n",
    "    #print(artificial_lower_bounds,artificial_upper_bounds)\n",
    "    \n",
    "    data = np.random.uniform(low=artificial_lower_bounds, high=artificial_upper_bounds, size=(num_instances, len(instance)))\n",
    "    df_artificial = pd.DataFrame(data, columns=X_test.columns)\n",
    "    df_artificial.loc[len(df_artificial)] = instance #Add original instance to the artificial set\n",
    "    X_test_art = df_artificial.values\n",
    "    test_dataset_df = df_artificial\n",
    "    test_dataset_df['target'] = clf.predict(df_artificial.values)\n",
    "    original_instance_df = pd.DataFrame(instance.reshape(1,-1), columns=X_test.columns)\n",
    "    original_instance_df['target'] = test_dataset_df['target'].values[-1]\n",
    "    #print(test_dataset_df['target'].unique())\n",
    "        \n",
    "    # Finding patterns classified as positive/negative\n",
    "    positive_indexes,negative_indexes = utility.find_indexes(clf, instance.reshape(1, -1), threshold=0)\n",
    "    \n",
    "    #Variables for results\n",
    "    #coverage_twostep = []\n",
    "    pos_exp_twostep = []\n",
    "    neg_exp_twostep = []\n",
    "    \n",
    "    #Generate Explanations for the patterns classified as negative\n",
    "    for idx in  negative_indexes:\n",
    "        \n",
    "        #Twostep\n",
    "        exp_ = docplex_explainer.twostep(\n",
    "                classifier = clf,\n",
    "                dual_coef = clf.dual_coef_,\n",
    "                support_vectors = clf.support_vectors_,\n",
    "                intercept = clf.intercept_,\n",
    "                lower_bound = lower_bound,\n",
    "                upper_bound = upper_bound,\n",
    "                data = (instance.reshape(1, -1)),\n",
    "                p = p,\n",
    "                positive = False)\n",
    "        neg_exp_twostep.append(exp_)\n",
    "        #coverage_twostep.append(len(mymetrics.calculate_coverage(test_dataset_df, exp_)))\n",
    "        twostep_coverages.append(len(mymetrics.calculate_coverage(test_dataset_df.drop(columns=['target']), exp_)))\n",
    "\n",
    "        #Anchors\n",
    "        prediction = class_names[anchors_explainer.predictor(instance.reshape(1, -1))[0]]\n",
    "        explanation = anchors_explainer.explain(instance.reshape(1, -1), threshold=1)\n",
    "        bounds = parse_explanation(explanation.anchor, feature_names, epsilon)\n",
    "        rsum = mymetrics.range_sum(bounds)\n",
    "        size = 0\n",
    "        for feature_name in feature_names:\n",
    "            for anchor in explanation.anchor:\n",
    "                if feature_name in re.split(r' <= | >= | < | > ', anchor):\n",
    "                    size += 1\n",
    "        coverage_df = mymetrics.calculate_coverage(test_dataset_df, bounds)\n",
    "        anchor_coverage = len(coverage_df)\n",
    "        error = len(coverage_df[coverage_df['target'] != prediction])\n",
    "\n",
    "        instance_coverage = len(mymetrics.calculate_coverage(original_instance_df, bounds))\n",
    "        if instance_coverage == 0:\n",
    "            anchor_coverage += 1\n",
    "\n",
    "        if anchor_coverage == 0:\n",
    "            print('WARNING')\n",
    "        \n",
    "        anchors_coverages.append(anchor_coverage)\n",
    "        anchors_errors.append(error)\n",
    "        anchors_rsums.append(anchors_rsum)\n",
    "        anchors_sizes.append(anchors_size)\n",
    "    \n",
    "    #Generate Explanations for the patterns classfied as positive\n",
    "    for idx in positive_indexes:\n",
    "        #Twostep\n",
    "        exp_ = docplex_explainer.twostep(\n",
    "                classifier = clf,\n",
    "                dual_coef = clf.dual_coef_,\n",
    "                support_vectors = clf.support_vectors_,\n",
    "                intercept = clf.intercept_,\n",
    "                lower_bound = lower_bound,\n",
    "                upper_bound = upper_bound,\n",
    "                data = (instance.reshape(1, -1)),\n",
    "                p = p,\n",
    "                positive = True)\n",
    "        pos_exp_twostep.append(exp_)\n",
    "        #coverage_twostep.append(len(mymetrics.calculate_coverage(test_dataset_df, exp_)))\n",
    "        twostep_coverages.append(len(mymetrics.calculate_coverage(test_dataset_df.drop(columns=['target']), exp_)))\n",
    "\n",
    "        #Anchors\n",
    "        prediction = class_names[anchors_explainer.predictor(instance.reshape(1, -1))[0]]\n",
    "        explanation = anchors_explainer.explain(instance.reshape(1, -1), threshold=1)\n",
    "        bounds = parse_explanation(explanation.anchor, feature_names, epsilon)\n",
    "        rsum = mymetrics.range_sum(bounds)\n",
    "        size = 0\n",
    "        for feature_name in feature_names:\n",
    "            for anchor in explanation.anchor:\n",
    "                if feature_name in re.split(r' <= | >= | < | > ', anchor):\n",
    "                    size += 1\n",
    "        coverage_df = mymetrics.calculate_coverage(test_dataset_df, bounds)\n",
    "        anchor_coverage = len(coverage_df)\n",
    "        error = len(coverage_df[coverage_df['target'] != prediction])\n",
    "\n",
    "        instance_coverage = len(mymetrics.calculate_coverage(original_instance_df, bounds))\n",
    "        if instance_coverage == 0:\n",
    "            anchor_coverage += 1\n",
    "\n",
    "        if anchor_coverage == 0:\n",
    "            print('WARNING')\n",
    "        \n",
    "        anchors_coverages.append(anchor_coverage)\n",
    "        anchors_errors.append(error)\n",
    "        anchors_rsums.append(anchors_rsum)\n",
    "        anchors_sizes.append(anchors_size)\n",
    "        \n",
    "    \n",
    "# Compute means and standard deviations\n",
    "def compute_mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "# Compute relative percentage differences\n",
    "def relative_percentage_diff(new, old):\n",
    "    if np.any(old == 0):\n",
    "        print(f'Warning: found possible division by zero')\n",
    "        return np.where(old != 0, ((new - old) / old) * 100, np.nan)\n",
    "    return ((new - old) / old) * 100\n",
    "\n",
    "# Ensure all lists are NumPy arrays\n",
    "anchors_coverages = np.array(anchors_coverages)\n",
    "twostep_coverages = np.array(twostep_coverages)\n",
    "anchors_errors = np.array(anchors_errors)\n",
    "\n",
    "(coverage_mean_anchors, coverage_std_anchors) = compute_mean_std(anchors_coverages)\n",
    "(coverage_mean_twostep, coverage_std_twostep) = compute_mean_std(twostep_coverages)\n",
    "\n",
    "coverage_mean_diff = relative_percentage_diff(coverage_mean_twostep, coverage_mean_anchors)\n",
    "coverage_std_diff = relative_percentage_diff(coverage_std_twostep, coverage_std_anchors)\n",
    "coverage_relative_pointwise = relative_percentage_diff(twostep_coverages, anchors_coverages)\n",
    "coverage_relative_mean = np.mean(coverage_relative_pointwise)\n",
    "coverage_relative_std = np.std(coverage_relative_pointwise)\n",
    "\n",
    "# Organize Data\n",
    "all_metrics_data = {\n",
    "    'Metric': ['Coverage'],\n",
    "    'ANCHORS_MEAN': [coverage_mean_anchors],\n",
    "    'ANCHORS_STD': [coverage_std_anchors],\n",
    "    'TWOSTEP_MEAN': [coverage_mean_twostep],\n",
    "    'TWOSTEP_STD': [coverage_std_twostep],\n",
    "    'MEAN_DIFF_%': [coverage_mean_diff],\n",
    "    'STD_DIFF_%': [coverage_std_diff],\n",
    "    'POINTWISE_MEAN_%': [coverage_relative_mean],\n",
    "    'POINTWISE_STD_%': [coverage_relative_std],\n",
    "    'ANCHORS_ERROR_RATE%': np.mean(anchors_errors/anchors_coverages)*100\n",
    "}\n",
    "# Display and save\n",
    "all_metrics_df = pd.DataFrame(all_metrics_data)\n",
    "display(all_metrics_df)\n",
    "all_metrics_df.to_csv(f'Anchors_vs_Twostep_Results/Artificial_{dataset_name}_results.csv', index=False)\n",
    "\n",
    "#Save Raw Metric Data\n",
    "raw_df = pd.DataFrame({\n",
    "    \"coverage_anchors\": anchors_coverages, \n",
    "    \"coverage_twostep\": twostep_coverages,\n",
    "    \"coverage_relative_%\": coverage_relative_pointwise\n",
    "})\n",
    "\n",
    "display(raw_df)\n",
    "raw_df.to_csv(f\"{dataset_name}_results/Artificial_coverage_raw_metric_data_{p}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99068b7-5c4f-483f-ab43-d08cd0afd2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
