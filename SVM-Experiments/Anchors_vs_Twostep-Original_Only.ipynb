{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191e9ff-5fda-43ea-8676-eaadee7fcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pulp import *\n",
    "from pulp import LpProblem, LpVariable, LpMinimize, LpInteger, lpSum, value, LpBinary,LpStatusOptimal\n",
    "import pulp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overwriting previously set objective.\")\n",
    "import utility\n",
    "import docplex.mp.model\n",
    "import docplex\n",
    "import docplex_explainer\n",
    "import mymetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import dill\n",
    "from alibi.explainers import AnchorTabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d45cc7-bde4-478e-bd98-777a198d5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'Iris':\n",
    "        # Load Dataset\n",
    "        dataset = datasets.load_iris()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        # Check if binary targets\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Banknote':\n",
    "        # Load Dataset\n",
    "        df = pd.read_csv('./datasets/banknote_authentication.csv')\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Blood_Transfusion':\n",
    "        df = pd.read_csv('./datasets/blood_transfusion.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Breast_Cancer':\n",
    "        dataset = datasets.load_breast_cancer()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Climate':\n",
    "        df = pd.read_csv('./datasets/climate_model_simulation_crashes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Glass':\n",
    "        df = pd.read_csv('./datasets/glass.csv')\n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Ionosphere':\n",
    "        df = pd.read_csv('./datasets/ionosphere.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Modeling':\n",
    "        df = pd.read_csv('./datasets/User_Knowledge_Modeling.csv')\n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x == 'Low' else 0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Parkinsons':\n",
    "        df = pd.read_csv('./datasets/parkinsons.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Pima':\n",
    "        df = pd.read_csv('./datasets/diabetes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Sonar':\n",
    "        df = pd.read_csv('./datasets/sonar.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Wine':\n",
    "        dataset = datasets.load_wine()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Vertebral-Column':\n",
    "        dataset_name = 'Vertebral-Column'\n",
    "        df = pd.read_csv('./datasets/column_2C.dat', sep=\" \", names=['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis','target'])\n",
    "        df['target']=np.where(df['target']=='AB',1,0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "        \n",
    "    else:\n",
    "        print(\"Incorrect dataset name\")\n",
    "\n",
    "def parse_explanation(explanation, feature_names, epsilon=1e-6):\n",
    "    bounds = [[0, 1] for _ in range(len(feature_names))]\n",
    "    conditions = explanation\n",
    "\n",
    "    for condition in conditions:\n",
    "        condition_no_space = condition.replace(' ', '')  # for regex matching\n",
    "        # Check for double inequality\n",
    "        match = re.match(r'(\\d+\\.?\\d*)\\s*(<|<=)\\s*([^\\s<>]+)\\s*(<|<=)\\s*(\\d+\\.?\\d*)', condition_no_space)\n",
    "        \n",
    "        if match:\n",
    "            value_1, op1, feature_token, op2, value_2 = match.groups()\n",
    "            value_1 = float(value_1)\n",
    "            value_2 = float(value_2)\n",
    "            lower_bound = value_1 if op1 == '<=' else value_1 + epsilon\n",
    "            upper_bound = value_2 if op2 == '<=' else value_2\n",
    "            upper_bound = upper_bound if op2 == '<=' else upper_bound - epsilon\n",
    "\n",
    "            for idx, feature in enumerate(feature_names):\n",
    "                if feature.replace(\" \", \"\") in feature_token:\n",
    "                    bounds[idx] = [lower_bound, upper_bound]\n",
    "                    break\n",
    "            continue  # go to next condition\n",
    "\n",
    "        # Fallback to single operator logic\n",
    "        for idx, feature in enumerate(feature_names):\n",
    "            if feature in condition:\n",
    "                cond_clean = condition.replace('<=', ' LESS_EQUAL ').replace('>=', ' GREATER_EQUAL ')\n",
    "                cond_clean = cond_clean.replace('<', ' < ').replace('>', ' > ')\n",
    "                tokens = cond_clean.split()\n",
    "\n",
    "                tokens = ['<=' if token == 'LESS_EQUAL' else token for token in tokens]\n",
    "                tokens = ['>=' if token == 'GREATER_EQUAL' else token for token in tokens]\n",
    "\n",
    "                operator = None\n",
    "                operator_pos = None\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if token in ['>', '>=', '<', '<=']:\n",
    "                        operator = token\n",
    "                        operator_pos = i\n",
    "                        break\n",
    "\n",
    "                value = None\n",
    "                if operator is not None and operator_pos is not None:\n",
    "                    for i in range(operator_pos + 1, len(tokens)):\n",
    "                        try:\n",
    "                            value = float(tokens[i])\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "                if value is not None:\n",
    "                    if operator == '>':\n",
    "                        bounds[idx] = [value + epsilon, 1]\n",
    "                    elif operator == '>=':\n",
    "                        bounds[idx] = [value, 1]\n",
    "                    elif operator == '<':\n",
    "                        bounds[idx] = [0, value - epsilon]\n",
    "                    elif operator == '<=':\n",
    "                        bounds[idx] = [0, value]\n",
    "                else:\n",
    "                    print(f\"Could not extract numeric value from condition: '{condition}'\")\n",
    "\n",
    "    return np.array(bounds)\n",
    "def train_anchors(dataset_name):\n",
    "    print(dataset_name)\n",
    "    clf = joblib.load(f'models/{dataset_name}_svm_model.pkl')\n",
    "    print(f'Loaded model')\n",
    "    X_train, X_test, y_train, y_test,feature_names, class_names = load_data(dataset_name)\n",
    "    if 'target' in feature_names:\n",
    "        feature_names = feature_names[feature_names != 'target']\n",
    "    print(f'feature names:\\n {feature_names}')\n",
    "    print(f'class names:\\n {class_names}')\n",
    "    predict_fn = lambda x: clf.predict(x)\n",
    "    explainer = AnchorTabular(predict_fn, feature_names)\n",
    "    explainer.fit(X_train, disc_perc=(25, 50, 75))\n",
    "    return explainer, feature_names, class_names\n",
    "    \n",
    "# Compute means and standard deviations\n",
    "def compute_mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "# Compute relative percentage differences\n",
    "def relative_percentage_diff(new, old):\n",
    "    if np.any(old == 0):\n",
    "        print(f'Warning: found possible division by zero')\n",
    "        return np.where(old != 0, ((new - old) / old) * 100, np.nan)\n",
    "    return ((new - old) / old) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc0668-ca10-43c9-af74-516342fef139",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name = ['Iris', 'Wine', 'Vertebral-Column', 'Pima', 'Parkinsons', 'Breast_Cancer', 'Blood_Transfusion', 'Ionosphere', 'Glass', 'Climate', 'Modeling', 'Banknote', 'Sonar']\n",
    "p_values = [0.25,0.5,0.75]\n",
    "for dataset_name in datasets_name:\n",
    "    for p in p_values:\n",
    "        path_anchors = f'./Anchors_results/{dataset_name}_results.csv' \n",
    "        path_twostep = dataset_name+ f'_results/results_{p}.csv'\n",
    "        path_twostep_brute = dataset_name+ f'_results/raw_metric_data_{p}.csv'\n",
    "        np.random.seed(50)\n",
    "        anchors_brute_results = pd.read_csv(path_anchors)\n",
    "        twostep_results = pd.read_csv(path_twostep)\n",
    "        twostep_results_brute = pd.read_csv(path_twostep_brute)\n",
    "        times_anchors = anchors_brute_results['time'].values\n",
    "        coverage_anchors = anchors_brute_results['coverage'].values\n",
    "        errors_anchors = anchors_brute_results['errors'].values\n",
    "        rsum_anchors = anchors_brute_results['rsum'].values\n",
    "        sizes_anchors = anchors_brute_results['size'].values\n",
    "        \n",
    "        times_twostep = twostep_results_brute['times_twostep'].values\n",
    "        coverage_twostep = twostep_results_brute['coverage_twostep'].values\n",
    "        rsum_twostep = twostep_results_brute['rsum_twostep'].values\n",
    "        feature_sizes_twostep = twostep_results_brute['feature_sizes_twostep'].values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Ensure all lists are NumPy arrays\n",
    "        times_twostep = np.array(times_twostep)\n",
    "        coverage_twostep = np.array(coverage_twostep)\n",
    "        sizes_anchors = np.array(sizes_anchors)\n",
    "        feature_sizes_twostep = np.array(feature_sizes_twostep)\n",
    "        rsum_anchors = np.array(rsum_anchors)\n",
    "        rsum_twostep = np.array(rsum_twostep)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Compute means and standard deviations\n",
    "        (time_mean_anchors, time_std_anchors) = compute_mean_std(times_anchors)\n",
    "        (time_mean_twostep, time_std_twostep) = compute_mean_std(times_twostep)\n",
    "        \n",
    "        (coverage_mean_anchors, coverage_std_anchors) = compute_mean_std(coverage_anchors)\n",
    "        (coverage_mean_twostep, coverage_std_twostep) = compute_mean_std(coverage_twostep)\n",
    "        \n",
    "        (sizes_mean_anchors, sizes_std_anchors) = compute_mean_std(sizes_anchors)\n",
    "        (sizes_mean_twostep, sizes_std_twostep) = compute_mean_std(feature_sizes_twostep)\n",
    "        \n",
    "        (rsum_mean_anchors, rsum_std_anchors) = compute_mean_std(rsum_anchors)\n",
    "        (rsum_mean_twostep, rsum_std_twostep) = compute_mean_std(rsum_twostep)\n",
    "        \n",
    "        # Compute relative percentage differences (Mean & Std)\n",
    "        time_mean_diff = relative_percentage_diff(time_mean_twostep, time_mean_anchors)\n",
    "        coverage_mean_diff = relative_percentage_diff(coverage_mean_twostep, coverage_mean_anchors)\n",
    "        \n",
    "        time_std_diff = relative_percentage_diff(time_std_twostep, time_std_anchors)\n",
    "        coverage_std_diff = relative_percentage_diff(coverage_std_twostep, coverage_std_anchors)\n",
    "        \n",
    "        sizes_mean_diff = relative_percentage_diff(sizes_mean_twostep, sizes_mean_anchors)\n",
    "        sizes_std_diff = relative_percentage_diff(sizes_std_twostep, sizes_std_anchors)\n",
    "        \n",
    "        rsum_mean_diff = relative_percentage_diff(rsum_mean_twostep, rsum_mean_anchors)\n",
    "        rsum_std_diff = relative_percentage_diff(rsum_std_twostep, rsum_std_anchors)\n",
    "        \n",
    "        # Compute pointwise relative differences\n",
    "        time_relative_pointwise = relative_percentage_diff(times_twostep, times_anchors)\n",
    "        coverage_relative_pointwise = relative_percentage_diff(coverage_twostep, coverage_anchors)\n",
    "        \n",
    "        sizes_relative_pointwise = relative_percentage_diff(feature_sizes_twostep, sizes_anchors)\n",
    "        rsum_relative_pointwise = relative_percentage_diff(rsum_twostep, rsum_anchors)\n",
    "        \n",
    "        # Compute pointwise means\n",
    "        time_relative_mean = np.mean(time_relative_pointwise) \n",
    "        coverage_relative_mean = np.mean(coverage_relative_pointwise)\n",
    "        sizes_relative_mean = np.mean(sizes_relative_pointwise)\n",
    "        rsum_relative_mean = np.mean(rsum_relative_pointwise)\n",
    "        \n",
    "        # Compute pointwise standard deviations\n",
    "        time_relative_std = np.std(time_relative_pointwise) \n",
    "        coverage_relative_std = np.std(coverage_relative_pointwise)\n",
    "        sizes_relative_std = np.std(sizes_relative_pointwise)\n",
    "        rsum_relative_std = np.std(rsum_relative_pointwise)\n",
    "        \n",
    "        # Organize Data\n",
    "        all_metrics_data = all_metrics_data = {\n",
    "            'Metric': ['Time', 'Size', 'Ranges_Sum', 'Coverage'],\n",
    "            'ANCHORS_MEAN': [time_mean_anchors, sizes_mean_anchors, rsum_mean_anchors, coverage_mean_anchors],\n",
    "            'ANCHORS_STD': [time_std_anchors, sizes_std_anchors, rsum_std_anchors, coverage_std_anchors],\n",
    "            'TWOSTEP_MEAN': [time_mean_twostep, sizes_mean_twostep, rsum_mean_twostep, coverage_mean_twostep],\n",
    "            'TWOSTEP_STD': [time_std_twostep, sizes_std_twostep, rsum_std_twostep, coverage_std_twostep],\n",
    "            'MEAN_DIFF_%': [time_mean_diff, sizes_mean_diff, rsum_mean_diff, coverage_mean_diff],\n",
    "            'STD_DIFF_%': [time_std_diff, sizes_std_diff, rsum_std_diff, coverage_std_diff],\n",
    "            'POINTWISE_MEAN_%': [time_relative_mean, sizes_relative_mean, rsum_relative_mean, coverage_relative_mean],\n",
    "            'POINTWISE_STD_%': [time_relative_std, sizes_relative_std, rsum_relative_std, coverage_relative_std],\n",
    "            'ANCHORS_ERROR_RATE_MEAN%': [None, None, None, np.mean(errors_anchors/coverage_anchors)*100],\n",
    "            'ANCHORS_ERROR_RATE_STD%': [None, None, None, np.std(errors_anchors/coverage_anchors)*100],\n",
    "            \n",
    "        }\n",
    "        # Display and save\n",
    "        all_metrics_df = pd.DataFrame(all_metrics_data)\n",
    "        display(all_metrics_df)\n",
    "        print(errors_anchors.sum())\n",
    "        all_metrics_df.to_csv(f'Anchors_vs_Twostep_Results/Original_{dataset_name}_{p}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7eca8c-fdd3-4d7d-aee1-0831ac43c975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99068b7-5c4f-483f-ab43-d08cd0afd2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfc4f3-b5a4-4fca-a078-cd1fecd76b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
