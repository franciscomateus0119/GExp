{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ae55c-7f5b-42da-9eef-3026529438d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fba2a-8bdb-4d7d-9446-929035622916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pulp import *\n",
    "from pulp import LpProblem, LpVariable, LpMinimize, LpInteger, lpSum, value, LpBinary,LpStatusOptimal\n",
    "import pulp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Overwriting previously set objective.\")\n",
    "import utility\n",
    "import docplex.mp.model\n",
    "import docplex\n",
    "import docplex_explainer\n",
    "import mymetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import dill\n",
    "from alibi.explainers import AnchorTabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be312d6-7cb4-4abe-b030-529eead83942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'Iris':\n",
    "        # Load Dataset\n",
    "        dataset = datasets.load_iris()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        # Check if binary targets\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Banknote':\n",
    "        # Load Dataset\n",
    "        df = pd.read_csv('./datasets/banknote_authentication.csv')\n",
    "        # Scale\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Blood_Transfusion':\n",
    "        df = pd.read_csv('./datasets/blood_transfusion.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Breast_Cancer':\n",
    "        dataset = datasets.load_breast_cancer()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Climate':\n",
    "        df = pd.read_csv('./datasets/climate_model_simulation_crashes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Glass':\n",
    "        df = pd.read_csv('./datasets/glass.csv')\n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x in [1, 2, 3] else 0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Ionosphere':\n",
    "        df = pd.read_csv('./datasets/ionosphere.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Modeling':\n",
    "        df = pd.read_csv('./datasets/User_Knowledge_Modeling.csv')\n",
    "        df['target'] = df['target'].apply(lambda x: 1 if x == 'Low' else 0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Parkinsons':\n",
    "        df = pd.read_csv('./datasets/parkinsons.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Pima':\n",
    "        df = pd.read_csv('./datasets/diabetes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Sonar':\n",
    "        df = pd.read_csv('./datasets/sonar.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "    elif dataset_name == 'Wine':\n",
    "        dataset = datasets.load_wine()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "\n",
    "    elif dataset_name == 'Vertebral-Column':\n",
    "        dataset_name = 'Vertebral-Column'\n",
    "        df = pd.read_csv('./datasets/column_2C.dat', sep=\" \", names=['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis','target'])\n",
    "        df['target']=np.where(df['target']=='AB',1,0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, df.columns.values, np.unique(targets)\n",
    "        \n",
    "    else:\n",
    "        print(\"Incorrect dataset name\")\n",
    "\n",
    "def parse_explanation(explanation, feature_names, epsilon=1e-6):\n",
    "    bounds = [[0, 1] for _ in range(len(feature_names))]\n",
    "    conditions = explanation\n",
    "\n",
    "    for condition in conditions:\n",
    "        condition_no_space = condition.replace(' ', '')  # for regex matching\n",
    "        # Check for double inequality\n",
    "        match = re.match(r'(\\d+\\.?\\d*)\\s*(<|<=)\\s*([^\\s<>]+)\\s*(<|<=)\\s*(\\d+\\.?\\d*)', condition_no_space)\n",
    "        \n",
    "        if match:\n",
    "            value_1, op1, feature_token, op2, value_2 = match.groups()\n",
    "            value_1 = float(value_1)\n",
    "            value_2 = float(value_2)\n",
    "            lower_bound = value_1 if op1 == '<=' else value_1 + epsilon\n",
    "            upper_bound = value_2 if op2 == '<=' else value_2\n",
    "            upper_bound = upper_bound if op2 == '<=' else upper_bound - epsilon\n",
    "\n",
    "            for idx, feature in enumerate(feature_names):\n",
    "                if feature.replace(\" \", \"\") in feature_token:\n",
    "                    bounds[idx] = [lower_bound, upper_bound]\n",
    "                    break\n",
    "            continue  # go to next condition\n",
    "\n",
    "        # Fallback to single operator logic\n",
    "        for idx, feature in enumerate(feature_names):\n",
    "            if feature in condition:\n",
    "                cond_clean = condition.replace('<=', ' LESS_EQUAL ').replace('>=', ' GREATER_EQUAL ')\n",
    "                cond_clean = cond_clean.replace('<', ' < ').replace('>', ' > ')\n",
    "                tokens = cond_clean.split()\n",
    "\n",
    "                tokens = ['<=' if token == 'LESS_EQUAL' else token for token in tokens]\n",
    "                tokens = ['>=' if token == 'GREATER_EQUAL' else token for token in tokens]\n",
    "\n",
    "                operator = None\n",
    "                operator_pos = None\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if token in ['>', '>=', '<', '<=']:\n",
    "                        operator = token\n",
    "                        operator_pos = i\n",
    "                        break\n",
    "\n",
    "                value = None\n",
    "                if operator is not None and operator_pos is not None:\n",
    "                    for i in range(operator_pos + 1, len(tokens)):\n",
    "                        try:\n",
    "                            value = float(tokens[i])\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "                if value is not None:\n",
    "                    if operator == '>':\n",
    "                        bounds[idx] = [value + epsilon, 1]\n",
    "                    elif operator == '>=':\n",
    "                        bounds[idx] = [value, 1]\n",
    "                    elif operator == '<':\n",
    "                        bounds[idx] = [0, value - epsilon]\n",
    "                    elif operator == '<=':\n",
    "                        bounds[idx] = [0, value]\n",
    "                else:\n",
    "                    print(f\"Could not extract numeric value from condition: '{condition}'\")\n",
    "\n",
    "    return np.array(bounds)\n",
    "def train_anchors(dataset_name):\n",
    "    print(dataset_name)\n",
    "    clf = joblib.load(f'models/{dataset_name}_svm_model.pkl')\n",
    "    print(f'Loaded model')\n",
    "    X_train, X_test, y_train, y_test,feature_names, class_names = load_data(dataset_name)\n",
    "    if 'target' in feature_names:\n",
    "        feature_names = feature_names[feature_names != 'target']\n",
    "    print(f'feature names:\\n {feature_names}')\n",
    "    print(f'class names:\\n {class_names}')\n",
    "    predict_fn = lambda x: clf.predict(x)\n",
    "    explainer = AnchorTabular(predict_fn, feature_names)\n",
    "    explainer.fit(X_train, disc_perc=(25, 50, 75))\n",
    "    return explainer, feature_names, class_names\n",
    "    \n",
    "# Compute means and standard deviations\n",
    "def compute_mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "# Compute relative percentage differences\n",
    "def relative_percentage_diff(new, old):\n",
    "    if np.any(old == 0):\n",
    "        print(f'Warning: found possible division by zero')\n",
    "        return np.where(old != 0, ((new - old) / old) * 100, np.nan)\n",
    "    return ((new - old) / old) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afadd68-a35e-484c-9777-4fcb5fda77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name = ['Iris', 'Wine', 'Vertebral-Column', 'Pima', 'Parkinsons', 'Breast_Cancer', 'Blood_Transfusion', 'Ionosphere', 'Glass', 'Climate', 'Modeling', 'Banknote', 'Sonar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80333cfc-3edf-4bec-a186-bdf5f7389471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'./Anchors_results/{datasets_name[0]}_explanations.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497a5eb-1700-4c32-9721-4c069f8b9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, columns, targets = load_data(datasets_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf117be-e858-437b-916e-9b615f2c8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_explanation(df['Explanation'].values[0:1],columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b728cb-dfb6-4c7a-be93-32bdb47a7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174038ca-46b9-42e2-ac2b-1956cf9bb704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
