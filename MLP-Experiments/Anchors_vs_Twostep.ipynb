{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191e9ff-5fda-43ea-8676-eaadee7fcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docplex\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utility\n",
    "import copy\n",
    "import mlp_explainer\n",
    "import mymetrics\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from milp import codify_network\n",
    "from teste import get_minimal_explanation\n",
    "from sklearn.metrics import classification_report\n",
    "from alibi.explainers import AnchorTabular\n",
    "import joblib\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d45cc7-bde4-478e-bd98-777a198d5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(50)\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'Iris':\n",
    "        dataset = datasets.load_iris()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = dataset.target\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Banknote':\n",
    "        df = pd.read_csv('./datasets/banknote_authentication.csv') \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        #targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        targets = df['target'].values\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Blood_Transfusion':\n",
    "        df = pd.read_csv('./datasets/blood_transfusion.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Breast_Cancer':\n",
    "        dataset = datasets.load_breast_cancer()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data) \n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = (utility.check_targets_0_1(np.where(dataset.target == dataset.target[0],0,1))).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Climate':\n",
    "        df = pd.read_csv('./datasets/climate_model_simulation_crashes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = df['target'].values\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Column':\n",
    "        df = pd.read_csv('./datasets/column_2C.dat', sep=\" \", names=['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis','target'])\n",
    "        df['target']=np.where(df['target']=='AB',1,0)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Glass':\n",
    "        df = pd.read_csv('./datasets/glass.csv')\n",
    "        unique_labels = sorted(df['target'].unique())\n",
    "        label_map = {original: new for new, original in enumerate(unique_labels)}\n",
    "        df['target'] = df['target'].map(label_map)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = df['target'].values\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Ionosphere':\n",
    "        df = pd.read_csv('./datasets/Ionosphere.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        lower_bound = scaled_df.min()\n",
    "        upper_bound = scaled_df.max()\n",
    "        print(lower_bound, upper_bound)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Modeling':\n",
    "        df = pd.read_csv('./datasets/User_Knowledge_Modeling.csv')\n",
    "        unique_labels = sorted(df['target'].unique()) \n",
    "        label_map = {original: new for new, original in enumerate(unique_labels)}\n",
    "        df['target'] = df['target'].map(label_map)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = df['target'].values\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Parkinson':\n",
    "        df = pd.read_csv('./datasets/parkinsons.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        lower_bound = scaled_df.min()\n",
    "        upper_bound = scaled_df.max()\n",
    "        print(lower_bound, upper_bound)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Pima':\n",
    "        df = pd.read_csv('./datasets/diabetes.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = (utility.check_targets_0_1(df.values[:,-1])).astype(np.int32)\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Sonar':\n",
    "        df = pd.read_csv('./datasets/sonar.csv')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df.values[:, :-1])\n",
    "        scaled_df = scaler.transform(df.values[:, :-1])\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns[:-1])\n",
    "        targets = df['target'].values\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    elif dataset_name == 'Wine':\n",
    "        dataset = datasets.load_wine()\n",
    "        df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dataset.data)\n",
    "        scaled_df = scaler.transform(dataset.data)\n",
    "        lower_bound = scaled_df.min()\n",
    "        upper_bound = scaled_df.max()\n",
    "        print(lower_bound, upper_bound)\n",
    "        df_scaled = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "        targets = dataset.target\n",
    "        df_scaled['target'] = targets\n",
    "        columns = df_scaled.columns\n",
    "        X_train, X_test, y_train, y_test = train_test_split(scaled_df, targets, test_size=0.75,random_state=50,stratify=targets)\n",
    "        return X_train, X_test, y_train, y_test, columns, np.unique(targets)\n",
    "    else:\n",
    "        print(\"Incorrect dataset name\")\n",
    "        \n",
    "def parse_explanation(explanation, feature_names, epsilon=1e-6):\n",
    "    bounds = [[0, 1] for _ in range(len(feature_names))]\n",
    "    conditions = explanation\n",
    "\n",
    "    for condition in conditions:\n",
    "        condition_no_space = condition.replace(' ', '')  # for regex matching\n",
    "        # Check for double inequality\n",
    "        match = re.match(r'(\\d+\\.?\\d*)\\s*(<|<=)\\s*([^\\s<>]+)\\s*(<|<=)\\s*(\\d+\\.?\\d*)', condition_no_space)\n",
    "        \n",
    "        if match:\n",
    "            value_1, op1, feature_token, op2, value_2 = match.groups()\n",
    "            value_1 = float(value_1)\n",
    "            value_2 = float(value_2)\n",
    "            lower_bound = value_1 if op1 == '<=' else value_1 + epsilon\n",
    "            upper_bound = value_2 if op2 == '<=' else value_2\n",
    "            upper_bound = upper_bound if op2 == '<=' else upper_bound - epsilon\n",
    "\n",
    "            for idx, feature in enumerate(feature_names):\n",
    "                if feature.replace(\" \", \"\") in feature_token:\n",
    "                    bounds[idx] = [lower_bound, upper_bound]\n",
    "                    break\n",
    "            continue  # go to next condition\n",
    "\n",
    "        # Fallback to single operator logic\n",
    "        for idx, feature in enumerate(feature_names):\n",
    "            if feature in condition:\n",
    "                cond_clean = condition.replace('<=', ' LESS_EQUAL ').replace('>=', ' GREATER_EQUAL ')\n",
    "                cond_clean = cond_clean.replace('<', ' < ').replace('>', ' > ')\n",
    "                tokens = cond_clean.split()\n",
    "\n",
    "                tokens = ['<=' if token == 'LESS_EQUAL' else token for token in tokens]\n",
    "                tokens = ['>=' if token == 'GREATER_EQUAL' else token for token in tokens]\n",
    "\n",
    "                operator = None\n",
    "                operator_pos = None\n",
    "                for i, token in enumerate(tokens):\n",
    "                    if token in ['>', '>=', '<', '<=']:\n",
    "                        operator = token\n",
    "                        operator_pos = i\n",
    "                        break\n",
    "\n",
    "                value = None\n",
    "                if operator is not None and operator_pos is not None:\n",
    "                    for i in range(operator_pos + 1, len(tokens)):\n",
    "                        try:\n",
    "                            value = float(tokens[i])\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "                if value is not None:\n",
    "                    if operator == '>':\n",
    "                        bounds[idx] = [value + epsilon, 1]\n",
    "                    elif operator == '>=':\n",
    "                        bounds[idx] = [value, 1]\n",
    "                    elif operator == '<':\n",
    "                        bounds[idx] = [0, value - epsilon]\n",
    "                    elif operator == '<=':\n",
    "                        bounds[idx] = [0, value]\n",
    "                else:\n",
    "                    print(f\"Could not extract numeric value from condition: '{condition}'\")\n",
    "\n",
    "    return np.array(bounds)\n",
    "\n",
    "def predict_fn(x):\n",
    "        return clf.predict(x, verbose=0)\n",
    "\n",
    "def train_anchors(dataset_name):\n",
    "    print(dataset_name)\n",
    "    clf = tf.keras.models.load_model(f'new_models/{dataset_name}.h5', compile=False)\n",
    "    print(f'Loaded model')\n",
    "    X_train, X_test, y_train, y_test,feature_names, class_names = load_data(dataset_name)\n",
    "    if 'target' in feature_names:\n",
    "        feature_names = feature_names[feature_names != 'target']\n",
    "    print(f'feature names:\\n {feature_names}')\n",
    "    print(f'class names:\\n {class_names}')\n",
    "    #predict_fn = lambda x: clf.predict(x)\n",
    "    explainer = AnchorTabular(predict_fn, feature_names)\n",
    "    explainer.fit(X_train, disc_perc=(25, 50, 75))\n",
    "    return explainer, feature_names, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc0668-ca10-43c9-af74-516342fef139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Sonar' #['Iris', 'Wine', 'Column', 'Pima', 'Parkinson', 'Breast_Cancer', 'Blood_Transfusion', 'Ionosphere', 'Glass', 'Climate', 'Modeling', 'Banknote', 'Sonar']\n",
    "path_anchors = f'./Anchors_results/{dataset_name}_results.csv' \n",
    "path_twostep = dataset_name+ '_results/results_0.25.csv'\n",
    "path_twostep_brute = dataset_name+ '_results/raw_metric_data_0.25.csv'\n",
    "np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe726b0-59ab-4c42-b659-b682a30b7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_brute_results = pd.read_csv(path_anchors)\n",
    "twostep_results = pd.read_csv(path_twostep)\n",
    "twostep_results_brute = pd.read_csv(path_twostep_brute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e6419-65d4-4ac8-827a-3b8f5046fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "twostep_results_brute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1439939-0672-4a21-85b7-7772f3be38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_brute_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2054936-c6bf-456e-93a2-2eb449236e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_anchors = anchors_brute_results['time'].values\n",
    "coverage_anchors = anchors_brute_results['coverage'].values\n",
    "errors_anchors = anchors_brute_results['errors'].values\n",
    "rsum_anchors = anchors_brute_results['rsum'].values\n",
    "sizes_anchors = anchors_brute_results['size'].values\n",
    "\n",
    "times_twostep = twostep_results_brute['times_twostep'].values\n",
    "coverage_twostep = twostep_results_brute['coverage_twostep'].values\n",
    "rsum_twostep = twostep_results_brute['rsum_twostep'].values\n",
    "feature_sizes_twostep = twostep_results_brute['sizes_twostep'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7eca8c-fdd3-4d7d-aee1-0831ac43c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means and standard deviations\n",
    "def compute_mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "# Compute relative percentage differences\n",
    "def relative_percentage_diff(new, old):\n",
    "    if np.any(old == 0):\n",
    "        print(f'Warning: found possible division by zero')\n",
    "        return np.where(old != 0, ((new - old) / old) * 100, np.nan)\n",
    "    return ((new - old) / old) * 100\n",
    "\n",
    "\n",
    "\n",
    "# Ensure all lists are NumPy arrays\n",
    "times_twostep = np.array(times_twostep)\n",
    "coverage_twostep = np.array(coverage_twostep)\n",
    "sizes_anchors = np.array(sizes_anchors)\n",
    "feature_sizes_twostep = np.array(feature_sizes_twostep)\n",
    "rsum_anchors = np.array(rsum_anchors)\n",
    "rsum_twostep = np.array(rsum_twostep)\n",
    "\n",
    "\n",
    "\n",
    "# Compute means and standard deviations\n",
    "(time_mean_anchors, time_std_anchors) = compute_mean_std(times_anchors)\n",
    "(time_mean_twostep, time_std_twostep) = compute_mean_std(times_twostep)\n",
    "\n",
    "(coverage_mean_anchors, coverage_std_anchors) = compute_mean_std(coverage_anchors)\n",
    "(coverage_mean_twostep, coverage_std_twostep) = compute_mean_std(coverage_twostep)\n",
    "\n",
    "(sizes_mean_anchors, sizes_std_anchors) = compute_mean_std(sizes_anchors)\n",
    "(sizes_mean_twostep, sizes_std_twostep) = compute_mean_std(feature_sizes_twostep)\n",
    "\n",
    "(rsum_mean_anchors, rsum_std_anchors) = compute_mean_std(rsum_anchors)\n",
    "(rsum_mean_twostep, rsum_std_twostep) = compute_mean_std(rsum_twostep)\n",
    "\n",
    "# Compute relative percentage differences (Mean & Std)\n",
    "time_mean_diff = relative_percentage_diff(time_mean_twostep, time_mean_anchors)\n",
    "coverage_mean_diff = relative_percentage_diff(coverage_mean_twostep, coverage_mean_anchors)\n",
    "\n",
    "time_std_diff = relative_percentage_diff(time_std_twostep, time_std_anchors)\n",
    "coverage_std_diff = relative_percentage_diff(coverage_std_twostep, coverage_std_anchors)\n",
    "\n",
    "sizes_mean_diff = relative_percentage_diff(sizes_mean_twostep, sizes_mean_anchors)\n",
    "sizes_std_diff = relative_percentage_diff(sizes_std_twostep, sizes_std_anchors)\n",
    "\n",
    "rsum_mean_diff = relative_percentage_diff(rsum_mean_twostep, rsum_mean_anchors)\n",
    "rsum_std_diff = relative_percentage_diff(rsum_std_twostep, rsum_std_anchors)\n",
    "\n",
    "# Compute pointwise relative differences\n",
    "time_relative_pointwise = relative_percentage_diff(times_twostep, times_anchors)\n",
    "coverage_relative_pointwise = relative_percentage_diff(coverage_twostep, coverage_anchors)\n",
    "\n",
    "sizes_relative_pointwise = relative_percentage_diff(feature_sizes_twostep, sizes_anchors)\n",
    "rsum_relative_pointwise = relative_percentage_diff(rsum_twostep, rsum_anchors)\n",
    "\n",
    "# Compute pointwise means\n",
    "time_relative_mean = np.mean(time_relative_pointwise) \n",
    "coverage_relative_mean = np.mean(coverage_relative_pointwise)\n",
    "sizes_relative_mean = np.mean(sizes_relative_pointwise)\n",
    "rsum_relative_mean = np.mean(rsum_relative_pointwise)\n",
    "\n",
    "# Compute pointwise standard deviations\n",
    "time_relative_std = np.std(time_relative_pointwise) \n",
    "coverage_relative_std = np.std(coverage_relative_pointwise)\n",
    "sizes_relative_std = np.std(sizes_relative_pointwise)\n",
    "rsum_relative_std = np.std(rsum_relative_pointwise)\n",
    "\n",
    "# Organize Data\n",
    "all_metrics_data = all_metrics_data = {\n",
    "    'Metric': ['Time', 'Size', 'Ranges_Sum', 'Coverage'],\n",
    "    'ANCHORS_MEAN': [time_mean_anchors, sizes_mean_anchors, rsum_mean_anchors, coverage_mean_anchors],\n",
    "    'ANCHORS_STD': [time_std_anchors, sizes_std_anchors, rsum_std_anchors, coverage_std_anchors],\n",
    "    'TWOSTEP_MEAN': [time_mean_twostep, sizes_mean_twostep, rsum_mean_twostep, coverage_mean_twostep],\n",
    "    'TWOSTEP_STD': [time_std_twostep, sizes_std_twostep, rsum_std_twostep, coverage_std_twostep],\n",
    "    'MEAN_DIFF_%': [time_mean_diff, sizes_mean_diff, rsum_mean_diff, coverage_mean_diff],\n",
    "    'STD_DIFF_%': [time_std_diff, sizes_std_diff, rsum_std_diff, coverage_std_diff],\n",
    "    'POINTWISE_MEAN_%': [time_relative_mean, sizes_relative_mean, rsum_relative_mean, coverage_relative_mean],\n",
    "    'POINTWISE_STD_%': [time_relative_std, sizes_relative_std, rsum_relative_std, coverage_relative_std],\n",
    "    'ANCHORS_ERROR_RATE_MEAN%': [None, None, None, np.mean(errors_anchors/coverage_anchors)*100],\n",
    "    'ANCHORS_ERROR_RATE_STD%': [None, None, None, np.std(errors_anchors/coverage_anchors)*100],\n",
    "    \n",
    "}\n",
    "# Display and save\n",
    "all_metrics_df = pd.DataFrame(all_metrics_data)\n",
    "display(all_metrics_df)\n",
    "print(errors_anchors.sum())\n",
    "all_metrics_df.to_csv(f'Anchors_vs_Twostep_Results/Original_{dataset_name}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1421f8-d3cb-4968-a086-04e44b6cb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "#df_artificial = pd.read_csv('datasets/artificial/'+f'{dataset_name}_artificial.csv')\n",
    "clf = tf.keras.models.load_model(f'new_models/{dataset_name}.h5', compile=False)\n",
    "loaded_bounds = np.load(f'bounds/{dataset_name}_data_bounds.npz')\n",
    "original_bounds = loaded_bounds['original_bounds']\n",
    "lower_bound, upper_bound = original_bounds[:, 0], original_bounds[:, 1]\n",
    "result_path = f'{dataset_name}_results'\n",
    "twostep_exps = np.load(f'{result_path}/twostep_explanations0.25.npz')['twostep_explanations']\n",
    "np.random.seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ef063-3dd6-465c-b6e9-fd7965cc2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1\n",
    "p = 0.25\n",
    "num_instances=100\n",
    "_1, _2, _3, _4,feature_names, class_names = load_data(dataset_name)\n",
    "if 'target' in feature_names:\n",
    "        feature_names = feature_names[feature_names != 'target']\n",
    "X_test = pd.read_csv(f'{dataset_name}_results/{dataset_name}_X_test.csv')\n",
    "epsilon = 1e-6\n",
    "# anchors_explainer, feature_names,class_names = train_anchors(dataset_name)\n",
    "# anchors_explainer.save(f'Anchors_explainers/{dataset_name}_model')\n",
    "anchors_explanations = pd.read_csv(f'./Anchors_results/{dataset_name}_explanations.csv')['Explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760a5ac-4f16-4414-b051-2679c495c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "twostep_coverages = []\n",
    "anchors_coverages = []\n",
    "anchors_errors = []\n",
    "anchors_rsums = []\n",
    "anchors_sizes = []\n",
    "for i, sample in enumerate(X_test.values[:,:-1]):\n",
    "    artificial_lower_bounds = sample-d\n",
    "    artificial_lower_bounds = np.maximum(artificial_lower_bounds, lower_bound)\n",
    "    artificial_upper_bounds = sample+d\n",
    "    artificial_upper_bounds = np.minimum(artificial_upper_bounds, upper_bound)\n",
    "    data = np.random.uniform(low=artificial_lower_bounds, high=artificial_upper_bounds, size=(num_instances, len(sample)))\n",
    "    df_artificial = pd.DataFrame(data, columns=X_test.columns[:-1])\n",
    "    df_artificial.loc[len(df_artificial)] = sample\n",
    "\n",
    "    test_dataset_df = df_artificial\n",
    "    test_dataset_df['target'] = np.argmax(clf.predict(df_artificial.values,verbose=0),axis=1)\n",
    "    original_instance_df = pd.DataFrame(sample.reshape(1,-1), columns=X_test.columns[:-1])\n",
    "    original_instance_df['target'] = test_dataset_df['target'].values[-1] #X_test.values[i,-1]\n",
    "    prediction = test_dataset_df['target'].values[-1]\n",
    "    twostep_coverages.append(len(mymetrics.calculate_coverage_with_tolerance(test_dataset_df.drop(columns=['target']),twostep_exps[i])))\n",
    "\n",
    "    #Anchors\n",
    "    explanation = ast.literal_eval(anchors_explanations[i])\n",
    "    bounds = parse_explanation(explanation, feature_names, epsilon)\n",
    "    rsum = mymetrics.range_sum(bounds)\n",
    "    size = 0\n",
    "    for feature_name in feature_names:\n",
    "        for anchor in explanation:\n",
    "            if feature_name in re.split(r' <= | >= | < | > ', anchor):\n",
    "                size += 1\n",
    "    #df_artificial['target'] = X_test.values[i,-1]\n",
    "    #display(test_dataset_df)\n",
    "    coverage_df = mymetrics.calculate_coverage_with_tolerance(test_dataset_df, bounds)\n",
    "    #print(bounds)\n",
    "    #display(coverage_df)\n",
    "    anchor_coverage = len(coverage_df)\n",
    "    error = len(coverage_df[coverage_df['target'] != prediction])\n",
    "\n",
    "    instance_coverage = len(mymetrics.calculate_coverage(original_instance_df, bounds))\n",
    "    if instance_coverage == 0:\n",
    "        anchor_coverage += 1\n",
    "\n",
    "    if anchor_coverage == 0:\n",
    "        print('WARNING')\n",
    "    \n",
    "    anchors_coverages.append(anchor_coverage)\n",
    "    anchors_errors.append(error)\n",
    "    anchors_rsums.append(rsum)\n",
    "    anchors_sizes.append(size)\n",
    "\n",
    "# Compute means and standard deviations\n",
    "def compute_mean_std(arr):\n",
    "    return np.mean(arr), np.std(arr)\n",
    "\n",
    "# Compute relative percentage differences\n",
    "def relative_percentage_diff(new, old):\n",
    "    if np.any(old == 0):\n",
    "        print(f'Warning: found possible division by zero')\n",
    "        return np.where(old != 0, ((new - old) / old) * 100, np.nan)\n",
    "    return ((new - old) / old) * 100\n",
    "\n",
    "# Ensure all lists are NumPy arrays\n",
    "anchors_coverages = np.array(anchors_coverages)\n",
    "twostep_coverages = np.array(twostep_coverages)\n",
    "#Fixing floating point error due to npz\n",
    "#twostep_coverages[twostep_coverages == 0] = 1\n",
    "anchors_errors = np.array(anchors_errors)\n",
    "\n",
    "(coverage_mean_anchors, coverage_std_anchors) = compute_mean_std(anchors_coverages)\n",
    "(coverage_mean_twostep, coverage_std_twostep) = compute_mean_std(twostep_coverages)\n",
    "\n",
    "coverage_mean_diff = relative_percentage_diff(coverage_mean_twostep, coverage_mean_anchors)\n",
    "coverage_std_diff = relative_percentage_diff(coverage_std_twostep, coverage_std_anchors)\n",
    "coverage_relative_pointwise = relative_percentage_diff(twostep_coverages, anchors_coverages)\n",
    "coverage_relative_mean = np.mean(coverage_relative_pointwise)\n",
    "coverage_relative_std = np.std(coverage_relative_pointwise)\n",
    "\n",
    "# Organize Data\n",
    "all_metrics_data = {\n",
    "    'Metric': ['Coverage'],\n",
    "    'ANCHORS_MEAN': [coverage_mean_anchors],\n",
    "    'ANCHORS_STD': [coverage_std_anchors],\n",
    "    'TWOSTEP_MEAN': [coverage_mean_twostep],\n",
    "    'TWOSTEP_STD': [coverage_std_twostep],\n",
    "    'MEAN_DIFF_%': [coverage_mean_diff],\n",
    "    'STD_DIFF_%': [coverage_std_diff],\n",
    "    'POINTWISE_MEAN_%': [coverage_relative_mean],\n",
    "    'POINTWISE_STD_%': [coverage_relative_std],\n",
    "    'ANCHORS_ERROR_RATE%': np.mean(anchors_errors/anchors_coverages)*100\n",
    "}\n",
    "# Display and save\n",
    "all_metrics_df = pd.DataFrame(all_metrics_data)\n",
    "display(all_metrics_df)\n",
    "all_metrics_df.to_csv(f'Anchors_vs_Twostep_Results/Artificial_{dataset_name}_results.csv', index=False)\n",
    "\n",
    "#Save Raw Metric Data\n",
    "raw_df = pd.DataFrame({\n",
    "    \"coverage_anchors\": anchors_coverages, \n",
    "    \"coverage_twostep\": twostep_coverages,\n",
    "    \"coverage_relative_%\": coverage_relative_pointwise\n",
    "})\n",
    "\n",
    "display(raw_df)\n",
    "raw_df.to_csv(f\"{dataset_name}_results/Artificial_coverage_raw_metric_data_{p}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a14fd4-8e5b-4406-9201-d98e1054756e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c38c41-fbaf-4bf9-b735-eccbcee4456e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
